# Unreal Engine C++ API Documentation Processing Pipeline

This project provides a robust, high-performance pipeline for extracting, processing, and organizing Unreal Engine 5.5 C++ API documentation. The scripts are designed for large-scale, parallel processing of HTML documentation, producing structured NDJSON outputs for further analysis or integration.

## Project Structure

- **scripts/**
  - **scanning/**: Scripts to scan and index all documentation files for each category (editor, developer, plugins, runtime).
  - **extraction/**: Scripts to extract structured entity data from indexed HTML files, outputting NDJSON for each category.
  - **processing/**: Scripts to parse, validate, and organize extracted data (classes, enums, constants, functions, class hierarchies).
  - **monitoring/**: Utilities for live progress monitoring and logging.
  - **utils/**: Validation, cleanup, and environment check scripts.
  - **main.py**: Orchestrates the full pipeline (scanning, extraction, processing) with robust step selection and dependency checks.
  - **requirements.txt**: All required Python dependencies.

- **json_output/**: Index files generated by scanning scripts (not versioned).
- **json_*_entities/**: NDJSON entity files generated by extraction scripts (not versioned).
- **json_constants/**, **json_enums/**, **json_functions/**: Outputs from processing scripts (not versioned).

## Key Features

- **Project-root-relative pathing** for all scripts and outputs.
- **Robust imports** and error handling.
- **NDJSON output** for all entity data (newline-delimited JSON, one entity per line).
- **High performance**: Uses `ThreadPoolExecutor` for parallel file processing and batch writes.
- **Live monitoring**: Real-time progress and error logging via `log_helper.py`.
- **Profiling and debug flags**: Use `--profile` and `--debug` for detailed timing and troubleshooting.
- **Validation and cleanup utilities**: Ensure data integrity and clean up outputs.
- **Strict rules**: Only top-level functions for multiprocessing, selectolax/BeautifulSoup for HTML parsing, and robust error messages.
- **Automatic step dependency management**: `main.py` will run all steps in sequence, re-checking for prerequisites after each phase.

## Usage

### 1. Install Python and Dependencies
- Use Python 3.10 or 3.11 (required for BeautifulSoup compatibility).
- Install dependencies:
  ```sh
  pip install -r scripts/requirements.txt
  ```

### 2. Run the Full Pipeline
From the project root:
```sh
python scripts/main.py
```
- This will run scanning, extraction, and processing in order.
- All outputs are written to the appropriate `json_*` folders.

### 3. Run Individual Steps
You can run only a specific phase:
- Scanning: `python scripts/main.py --scanning`
- Extraction: `python scripts/main.py --extraction`
- Processing: `python scripts/main.py --processing`

### 4. Debugging and Profiling
- Add `--debug` to any extraction script for verbose output.
- Add `--profile` to any script or to `main.py` for timing information.

### 5. Validation and Cleanup
- Use scripts in `scripts/utils/` to validate project structure or clean up outputs.

## Version Control
- Only the `scripts/` directory and `.gitignore` are tracked in version control.
- All outputs and generated files are ignored (see `.gitignore`).

## Troubleshooting
- If a step is skipped due to missing prerequisites, simply re-run `main.py`â€”it will pick up where it left off.
- For extraction hangs, use debug flags and per-file timeouts to isolate problematic files.
- All errors are logged to `main_extraction_error.log` in the project root.

## Contact
For questions or contributions, please open an issue or pull request.
